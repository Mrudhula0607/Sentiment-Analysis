# -*- coding: utf-8 -*-
"""code final.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1nMyC6a9nHf5eq-DCeC6bPPVfQAL877vG

# **STEP 1: GETTING THE DATA**
"""

pip install nltk==3.3

import nltk
import matplotlib.pyplot as plt

nltk.download('twitter_samples')

from nltk.corpus import twitter_samples

positive_tweets = twitter_samples.strings('positive_tweets.json')
negative_tweets = twitter_samples.strings('negative_tweets.json')

print(positive_tweets[5])

print(negative_tweets[0])

"""# **STEP 2: PREPROCESSING THE DATA TO USEABLE FORM**

**TOKENISATION**:
"""

nltk.download('punkt')
from nltk.tokenize import word_tokenize

positive_tokenised = twitter_samples.tokenized('positive_tweets.json')
negative_tokenised = twitter_samples.tokenized('negative_tweets.json')

print(positive_tokenised[6])
print(negative_tokenised[0])

"""**LEMMETISING:**"""

nltk.download('wordnet')

from nltk.stem import WordNetLemmatizer 
lemmatizer = WordNetLemmatizer()

positive_lemmatized = [None]*len(positive_tweets)
for i in range (len(positive_tokenised)):
  positive_lemmatized[i] = [None] * len(positive_tokenised[i])
  for j in range (len(positive_tokenised[i])):
      positive_lemmatized[i][j] = lemmatizer.lemmatize(positive_tokenised[i][j])

negative_lemmatized = [None]*len(negative_tweets)
for i in range (len(negative_tokenised)):
  negative_lemmatized[i] = [None] * len(negative_tokenised[i])
  for j in range (len(negative_tokenised[i])):
      negative_lemmatized[i][j] = lemmatizer.lemmatize(negative_tokenised[i][j])

print(positive_lemmatized[0])
print(negative_lemmatized[0])

"""**REMOVING STOP WORDS:**"""

nltk.download('stopwords')

from nltk.corpus import stopwords
stop_words = stopwords.words('english')

extra_words = ['im','co', 'hi', 'hey', 'hello', 'hai', 'someone', 'I am', 'see', 'thing', 'need', 'know', 'that', 'this', 'say', 'come', 'got', 'get', "I'm", '...', '..', 'I', 'i', 'ME', 'u', 'Hi', 'Hey', 'via', "I'll"]

import string
punctuation = string.punctuation
punctuation = list(punctuation)
print(punctuation)

stop_words = stop_words + extra_words + punctuation

print(stop_words)

positive_cleaned = [None]*len(positive_tweets)
for i in range(len(positive_tweets)):
  ans = []
  for w in positive_tokenised[i]:
    if(w.startswith('http:/') or w.startswith('https:/') or w.startswith('@')):
      continue;
    if w not in stop_words: 
        ans.append(w)
  positive_cleaned[i] = ans
print(positive_cleaned[6])

negative_cleaned = [None]*len(negative_tweets)
for i in range(len(negative_tweets)):
  ans = []
  for w in negative_tokenised[i]:
    if(w.startswith('http:/') or w.startswith('https:/') or w.startswith('@')):
      continue; 
    if w not in stop_words: 
        ans.append(w)
  negative_cleaned[i] = ans
print(negative_cleaned[0])

positive_string_cleaned = [None] * len(positive_cleaned)
for i in range(len(positive_cleaned)):
  positive_string_cleaned[i] = " ".join(positive_cleaned[i])

negative_string_cleaned = [None] * len(negative_cleaned)
for i in range(len(negative_cleaned)):
  negative_string_cleaned[i] = " ".join(negative_cleaned[i])

print(positive_string_cleaned[6])

"""**BUILDING THE DATASET:**"""

import pandas as pd

positive_data = pd.DataFrame(positive_tweets)
negative_data = pd.DataFrame(negative_tweets)

positive_data = positive_data.rename(columns={0:'Raw Tweets'})
negative_data = negative_data.rename(columns={0:'Raw Tweets'})

print(positive_data)
print(negative_data)

positive_string_cleaned = [None] * len(positive_cleaned)
for i in range(len(positive_cleaned)):
  positive_string_cleaned[i] = " ".join(positive_cleaned[i])

negative_string_cleaned = [None] * len(negative_cleaned)
for i in range(len(negative_cleaned)):
  negative_string_cleaned[i] = " ".join(negative_cleaned[i])

positive_data['Cleaned'] = positive_string_cleaned
negative_data['Cleaned'] = negative_string_cleaned

positive_data['Class'] = 'Positive'
negative_data['Class'] = 'Negative'

print(positive_data)
print(negative_data)

frames = [positive_data, negative_data]
dataset = pd.concat(frames)
print(dataset)

"""# **STEP 3: VISUALISING THE DATA**

**DETERMINING WORD DENSITY**
"""

def get_all_words(cleaned_tokens_list):
    for tokens in cleaned_tokens_list:
        for token in tokens:
            yield token

positive_words = get_all_words(positive_cleaned)
negative_words = get_all_words(negative_cleaned)

from nltk import FreqDist

positive_frequency = FreqDist(positive_words)
print(positive_frequency.most_common(20))

negative_frequency = FreqDist(negative_words)
print(negative_frequency.most_common(20))

positive_frequency.plot(30,cumulative=False)
plt.show()

negative_frequency.plot(30,cumulative=False)
plt.show()

"""**WORD CLOUD**"""

from wordcloud import WordCloud

text = " ".join(word for word in positive_string_cleaned)
wordcloud = WordCloud(max_font_size=50, max_words=100, background_color="white", stopwords=stopwords.words("english")).generate(text)

plt.figure()
plt.imshow(wordcloud, interpolation="bilinear")
plt.axis("off")
plt.show();

text = " ".join(word for word in negative_string_cleaned)
wordcloud = WordCloud(max_font_size=50, max_words=100, background_color="white", stopwords=stopwords.words("english")).generate(text)

plt.figure()
plt.imshow(wordcloud, interpolation="bilinear")
plt.axis("off")
plt.show();

"""# **STEP 4: BUILDING THE MULTINOMIAL NAIVE BAYESIAN CLASSIFIER**

**DIVIDING THE DATA INTO TRAIN AND TEST SET**
"""

from sklearn.model_selection import train_test_split

x = dataset[dataset.columns[1]]
x.head()
y = dataset[dataset.columns[2]]
y.head()

x_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.2)

x_train.head()

y_train.head()

x_test.head()

y_test.head()

"""**WORD DENSITY**"""

def get_all_words(cleaned_tokens_list):
    for tokens in cleaned_tokens_list:
        for token in tokens:
            yield token

positive_words = get_all_words(positive_cleaned)
negative_words = get_all_words(negative_cleaned)

from nltk import FreqDist

positive_frequency = FreqDist(positive_words)
negative_frequency = FreqDist(negative_words)

df_positive_frequency = pd.DataFrame.from_dict(positive_frequency,orient = 'index')
df_positive_frequency.columns = ['Frequency']
df_positive_frequency.index.name = 'Term'

print(df_positive_frequency.head())

print(df_positive_frequency['Frequency']['top'])

df_negative_frequency = pd.DataFrame.from_dict(negative_frequency,orient = 'index')
df_negative_frequency.columns = ['Frequency']
df_negative_frequency.index.name = 'Term'

print(df_negative_frequency.head())

print(df_negative_frequency['Frequency']['kids'])

"""**FUNCTION TO IMPLEMENT ALGORITHM**"""

def clean(custom_tweet):
  #tokenise
  custom_tweet_tokenised = custom_tweet.split()
  
  #lemmatise
  custom_tweet_lemmatized = [None]*len(custom_tweet_tokenised)
  for i in range(len(custom_tweet_tokenised)):
    custom_tweet_lemmatized[i] = lemmatizer.lemmatize(custom_tweet_tokenised[i])

  #remove stop words
  custom_tweet_cleaned = []
  for w in custom_tweet_lemmatized:
    if(w.startswith('http:/') or w.startswith('https:/') or w.startswith('@')):
      continue;
    if w not in stop_words: 
        custom_tweet_cleaned.append(w)

  return custom_tweet_cleaned

def calculate(cleaned_custom_tweet, class_name):
  #class probability
  temp_dict = y_train.to_dict()
  class_count = 0
  for i in temp_dict:
    if(temp_dict[i] == class_name):
      class_count += 1
  class_prob = class_count/y_train.size

  #conditinal probability for each word
  conditional_prob = [None]*len(cleaned_custom_tweet)
  v = df_positive_frequency.size + df_negative_frequency.size
  total_positive_words = len(list(positive_words))
  total_negative_words = len(list(negative_words))

  for i in range(len(cleaned_custom_tweet)):
    value = 1
    #positive tweet set
    if class_name == 'Positive':
      if cleaned_custom_tweet[i] in df_positive_frequency['Frequency']:
        value += df_positive_frequency['Frequency'][cleaned_custom_tweet[i]] 
      conditional_prob[i] = value/(total_positive_words+v)
    #negative tweet set
    elif class_name == 'Negative':
      if cleaned_custom_tweet[i] in df_negative_frequency['Frequency']:
        value += df_negative_frequency['Frequency'][cleaned_custom_tweet[i]] 
    conditional_prob[i] = value/(total_negative_words+v)

  #calculating answer
  answer = class_prob
  for i in conditional_prob:
    answer *= i
  return answer

def MultinomialNaiveBayesianClassifier(custom_tweet):
  #cleaning the data
  cleaned_custom_tweet = clean(custom_tweet)
  #print("Cleaned tweet is: ", cleaned_custom_tweet)

  #finding the best suited class
  ans_df = pd.DataFrame(columns=['Class', 'Probability'])
  classes = ['Positive', 'Negative']
  ans_df['Class'] = classes
  for i in range(len(classes)):
    ans_df['Probability'][i] = calculate(cleaned_custom_tweet, classes[i])
  
  #returning the class
  if(ans_df['Probability'][0] > ans_df['Probability'][1]):
    return classes[0]
  else:
    return classes[1]

"""**TESTING THE MODEL WITH TESTING DATA**"""

y_pred =[None]*x_test.size
for i in range(x_test.size):
  y_pred[i] = MultinomialNaiveBayesianClassifier(x_test.iloc[i])
print(y_pred)

"""**ACCURACY**"""

from sklearn.metrics import accuracy_score

y_test_list = list(y_test)
accuracy = accuracy_score(y_test_list, y_pred)*100
print("Accuracy: ", accuracy, "%")

"""**CHECKING WITH CUSTOM TWEET**"""

custom_tweet = input("Enter the custom tweet: ")
ans = MultinomialNaiveBayesianClassifier(custom_tweet)
print("The given tweet is ", ans)